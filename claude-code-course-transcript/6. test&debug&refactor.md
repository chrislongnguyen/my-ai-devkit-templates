The codebase is now missing tests
0:02
for evaluating the RAG pipeline of the chatbot.
0:06
Let's implement these tests and use them to debug an error
0:09
that the chatbot is having processing the queries, and finally, refactor
0:14
how the chatbot handles its tool use. Let's dive in.
0:18
So far we've seen how to get up to speed with codebases
0:21
and implement features to build a slightly more powerful experience.
0:26
Now let's go ahead and imagine that it's
0:28
been some time since we've worked on this
0:29
application. We're coming back and we want to start using it again.
0:32
So let's hop back to our application
0:34
and ask for some details on what's covered in Lesson 5.
0:38
We might expect to see a response
0:40
that gives us information, but right now, something's wrong.
0:43
It might be tempting to just
0:45
copy this error, put it into Claude,
0:46
take a screenshot, hope it solves the problem.
0:48
But we're going to do something a little bit different.
0:51
Instead of just getting the answer that we might want
0:54
or leading Claude on some wild goose chase.
0:58
We're going to take a bit more of a methodical approach here.
1:00
We know that things are wrong in our application,
1:02
but we also know that we don't have too many tests here
1:05
to programmatically verify that that's the case.
1:09
So what we're going to do is put in a prompt
1:11
that not only mentions what the error is,
1:13
but also specifies where we need to write tests.
1:17
If we go back to our code base,
1:19
the error we're looking at could be
1:21
from a variety of different Python files.
1:24
In our AIGenerator, this is where we handle interactions with Anthropic's API.
1:28
And there could be problems in the prompt
1:31
or even some of the logic
1:32
we have for getting this part working.
1:35
In our rag_system.py,
1:36
this is the orchestrator for everything that's happening with Retrieval Augmented Generation.
1:42
And in our search_tools, here's where we define the underlying tool
1:46
definition where there could potentially be problems.
1:49
So what we're going to ask Claude to do
1:51
is write tests for these specific files just to start.
1:54
We're then going to ask Claude to run those tests
1:56
and through that, verify what is not working and make
1:59
the appropriate fix. What's so powerful about this approach
2:03
is by starting with our tests, we
2:05
can start to build a robust foundation
2:07
for building more off of the codebase
2:09
and understanding when things go wrong, why they're failing.
2:12
So let's open up Claude and put in a prompt.
2:15
Since this is a little bit more challenging,
2:18
we're also going to ask Claude to think a lot.
2:22
This is going to trigger Claude's ability
2:24
to enable extended thinking
2:26
and allocate a few more tokens
2:28
towards the thinking process. we're also going to see this process
2:32
and if something doesn't appear right, we
2:33
can always stop Claude in its tracks.
2:35
We'll go ahead and make sure the plan mode is on
2:38
because first we want to make sure before Claude starts writing tests,
2:41
it understands what it needs to
2:43
do and we can approve that process.
2:45
As we start to read the files necessary,
2:47
we're going to see Claude's thinking process.
2:49
We're not only going to see the files that we're reading,
2:52
but also what it needs to examine and what it should do.
2:57
So let's go see what Claude's plan has. It's
2:59
It seems like there's some kind of configuration issue,
3:02
maybe some complex tool calling with failure points
3:05
and some limited error propagation between components.
3:09
So it's possible that the error is just getting caught somewhere.
3:12
It's then going to propose a structure for tests
3:14
based on these files, which looks good to us.
3:18
use the pytest framework and mock whatever
3:19
it is in ChromaDB that we need
3:21
to get some unit tests and integration tests up and running.
3:26
You can see right now it's starting to make
3:28
a folder for my tests, where it'll be running.
3:29
writing these. It's a great start.
3:32
We'll go ahead and make sure we add
3:34
any necessary dependencies here as well with UV.
3:38
Let's go install our dependencies,
3:40
make sure we have pytest working as expected, and that looks good.
3:42
We're also going to see the thinking part
3:44
here to make sure that we're doing what's expected.
3:46
As we start to write these particular tests,
3:48
we can start exploring the code that's been written for us.
3:51
Here we can see tests for the course search tool,
3:54
fixtures and mocks that we need to make.
3:56
This looks like a good start. We'll
3:58
see the same thing for our AI generator.
3:59
and our RAG system and our vector store.
4:02
It seems like it's identified a critical
4:03
config issue which may solve the bug,
4:06
but writing these tests is going to give us complete assurance
4:08
that this is actually the problem
4:10
and that this is the correct fix.
4:20
Now that we've created these tests, let's run them using UV.
4:23
Let's go make sure we have the correct dependencies
4:25
so that we can run those tests as expected.
4:27
So from its finding, it's telling us that it's
4:29
seems like there's something wrong with these max results
4:33
or the number of chunks that were
4:34
returning when we perform our vector search. So let's see
4:38
what we're getting when we take a look at our MAX_RESULTS.
4:40
And this confirms the issue that we have.
4:42
For some reason, this happens to be zero.
4:44
Once we go ahead and change this, we should expect
4:46
that not only our tests are working as expected,
4:49
but that the results we get are complete.
4:51
We can verify this by taking a look and
4:53
making sure we get the file that we expect.
4:55
Now that Claude Code is wrapping up,
4:57
it can provide a comprehensive summary of its
4:59
findings, which I can wait to
5:01
see or if I feel good now,
5:02
I can stop Claude in its tracks.
5:05
But I'll take a look at what
5:06
it gives me. It's completed its debugging.
5:08
It's found the critical issue, and it's created a few tests,
5:11
as well as some infrastructure to
5:12
keep running tests as we go forward.
5:15
Let's go see if this worked as expected.
5:16
Back in the browser, I'll start a new conversation,
5:19
and let's ask that same question again and
5:21
see if we get the result as expected.
5:23
Instead of the query failing, we should expect to see information
5:27
just about the lesson and that's exactly what we did.
5:29
we have here. So not only have
5:31
we fixed the bug, we built for ourselves
5:32
a powerful infrastructure to keep running the application off
5:34
of and make sure that as we make new changes,
5:37
things are not breaking and we don't know why.
5:40
Now that we got this application working,
5:42
let's talk about a little bit of a refactor
5:44
that I'd like to make in the codebase.
5:46
Let's clear and start again with a new feature.
5:49
So here in our ai_generator.py,
5:51
we specify that we want one search per query maximum.
5:55
While this leads to the expected behavior for relatively simple
6:00
when we want to start doing more complex queries,
6:02
comparing different courses, comparing their outlines,
6:05
we're going to need more than one tool call.
6:08
So what we're going to need is some kind of environment
6:10
where we can either iteratively go through all the tools necessary
6:14
or recursively solve that problem.
6:17
If you're comfortable with this code and ecosystem,
6:20
you can take a look at the class that we have here,
6:22
which is a relatively simple way
6:24
to talk directly to Anthropic models
6:26
using our SDK, set your base_params and
6:29
generate a response. But you can see
6:31
here, as we build our system prompt,
6:33
as we figure out the messages necessary and any tools,
6:37
there's no iteration and back and forth to accumulate
6:40
our tools necessary and have a
6:42
multi-turn conversation with multiple tools being used.
6:46
So let's refactor that. I've got a pretty long prompt here,
6:49
so I'm going to go ahead and make
6:50
a new markdown file and let's go call this
6:53
backend-tool-refactor.md
6:56
In this file, I want to walk through the
6:58
prompt that I have where I'm
6:59
going to ask Ask Claude to refactor
7:01
the backend ai_generator.py
7:03
to support two calls in separate rounds.
7:06
The current behavior is what I'm describing
7:08
as well as the desired behavior.
7:10
While this might not be required,
7:12
it's often helpful, as we've seen,
7:15
to give Claude as much information as possible,
7:17
especially when the tasks are a little bit more complex.
7:20
We're also going to give Claude an example flow.
7:23
Something like search for a course that
7:25
discusses the same topic as another course.
7:28
Just to give Claude a sense of what needs to be done.
7:30
Notice here, there are a couple of different
7:32
tools that need to be used as expected.
7:34
We'll give Claude some requirements,
7:36
and then a couple of notes as well
7:38
to make sure we're doing the right thing.
7:40
We're going to make sure that
7:42
we write tests that verify external behavior
7:44
instead of worrying about internal state details.
7:47
Or we're also going to ask Claude to
7:49
do is figure out a couple different plans.
7:52
Don't implement any code,
7:53
but dispatch two subagents to brainstorm potential options.
7:58
In this situation, I'm not exactly sure what the optimal refactoring is.
8:03
So instead of letting Claude Code decide just one
8:05
option, I'm actually going to give it the opportunity
8:07
to figure out multiple options in parallel.
8:11
So let's use that particular prompt
8:12
and I'll turn off auto accept to make sure
8:15
that as I'm going through, I can
8:16
confirm each of the changes that
8:18
I want. So let's run that prompt.
8:20
We should expect to see the two parallel subagents
8:23
being dispatched using a tool called task.
8:26
And as we see those agents dispatched,
8:28
we'll start to see two plans
8:30
for how we can solve this problem.
8:32
It's then up to us to decide which one to implement.
8:35
We can see that both of these are operating in parallel,
8:38
reading across files and figuring out two different approaches for this refactor.
8:42
So it looks like it's come back with
8:44
some recommendations for how best to tackle this.
8:46
One approach is iterative, another more comprehensive.
8:50
We can see here that Claude is actually going
8:52
to give us an option to choose either option B
8:55
for better long-term maintainability or option A for a safer implementation.
8:59
scroll up more. If I scroll up more, I
9:01
can get some details into what it's trying to do.
9:04
One option supporting some iteration for handling tools,
9:07
the other for a slightly more complicated implementation
9:11
for multi-round logic with different helper methods for that process.
9:15
From first glance, it seems like approach A is a bit simpler.
9:18
So why don't we start with
9:19
that? I'm going to select approach A,
9:20
but I'm also now going to enable plan mode
9:23
to make sure I have a comprehensive plan and
9:26
can accept before making the changes that I want.
9:29
Let's implement approach A.
9:30
and get a more detailed plan to verify
9:32
that this is exactly what we want to do.
9:34
We know that there needs to be some modification
9:36
in the way that which we call these tools,
9:39
but let's make sure this is all being
9:40
done in the right place before we go ahead
9:42
and have Claude Code write the solution for us.
9:45
Now let's take a look at what's being done here.
9:47
We'll update our method signature for a maximum number of rounds,
9:50
update our system prompt, and here where the
9:53
core refactoring is done in our handle tool execution.
9:56
If this is done as expected, we
9:57
should see the user query, we should see
9:59
multiple tools being called and then a final response.
10:03
This shouldn't modify anything else and should solve the problem for us.
10:07
We can see here that this is backwards compatible,
10:10
and we're not changing the internal RAG system or any API endpoints.
10:14
So let's proceed and see if
10:16
this can solve our problem for us.
10:17
As we see what's being implemented,
10:20
we can see changes to existing methods,
10:22
and most importantly, we can see here that
10:25
we're updating the tests and running the tests
10:27
to verify that the implementation works correctly.
10:31
Instead of context switching to the browser
10:33
or asking other people to test,
10:35
I can do this all from inside the terminal
10:37
in Claude Code.
10:45
So we'll see here, we're going to update our system prompt,
10:47
make sure that our tests are as expected.
10:50
And then we'll add a test to make
10:51
sure the sequential tool calling works as expected.
10:54
Let's run our tests, make sure they're passing as expected.
10:57
Let's verify in the browser, this is doing what
10:59
if you want. Let's try asking first
11:02
for a details of a course's lesson.
11:06
So here we can see
11:07
that not only are we getting this information,
11:10
but also the title here as well, as expected.
11:12
We're getting information about this lesson,
11:14
as well as some of the sections and topics discussed.
11:18
Now let's do something a little more complex.
11:20
What's so special about seeing the title here is
11:22
that we don't get that with just one tool call.
11:25
The first tool call just gives us an outline of the course.
11:29
The second tool call gives us that detail
11:31
of the particular lesson and in our case the title.
11:34
Now let's ask, Are there any other courses that cover
11:37
the same topic as Lesson 5 of the MCP course?
11:42
In order to do this, we're going
11:44
to need to make multiple tool calls
11:45
to get information about this particular MCP course
11:48
and outline and then information about other courses and their outlines
11:53
to see if there's any overlap.
11:55
Unfortunately, it looks like there are no other courses
11:58
that cover building an MCP client, which does seem accurate.
12:01
In this lesson, we've seen how to use Claude Code to
12:05
not only fix bugs, but write tests throughout the entire process.
12:09
We built for ourselves a solid foundation to keep coding on
12:12
and made a nice refactor to get more complicated query answers appropriately.
12:17
In the next lesson, we'll improve our
12:20
productivity by running multiple sessions of Claude Code
12:23
and making sure we don't have overlaps or
12:26
overwrites using Git worktrees. I'll see you there.